"""
Torch dataset object for synthetically rendered spatial data.
"""
'''
This code was generated by @Sidharth, This code contains the creation of dataset on fly

Input : dataset_dir, sr, split
Output : inputs #(1, 3, fs*T), target_audio #(1,1,fs*T)

'''
import torch.nn.functional as F
import os, glob
from pathlib import Path
import random
import scipy.ndimage
from torch.utils.data import Dataset
import pandas as pd
import torch
import torchaudio
import tqdm
from multiprocessing import Pool
# from src.datasets.augmentations.audio_augmentations import AudioAugmentations
import numpy as np
import subprocess
from gpuRIRsimulateRIR import PRASimulator
# from scipy.signal import fftconvolve
import pdb
import soundfile as sf
'''
LibriTTS
dir : /scr/LibriTTS
relevant folders : dev-clean, test-clean, train-clean-360
'''



#untar the libritts files to the /scr folder
sh_path = '/mmfs1/gscratch/intelligentsystems/sidharth/codebase/shcodes/tar_libritts.sh'
if os.path.exists('/scr/LibriTTS')==False:
    subprocess.run(["sh", sh_path]) #creating libriTTS train, test and dev

class LibriTTS_spatial_audio(Dataset):
    def __init__(self, dataset_dir, sr, split, augmentations = [], samples_per_epoch=20000) -> None:
        super().__init__()
        assert split in ['train', 'val', 'test'], \
            "`split` must be one of ['train', 'val', 'test']"

        self.dataset_dir = Path(dataset_dir) #/scr/LibriTTS/
        self.split = split #/train or valid
        self.sr = sr
        self.duration = 5
        # Data augmentation
        # self.perturbations = AudioAugmentations(augmentations)
        self.samples = sorted(list(self.dataset_dir.glob('**/*.wav')))
        # self.samples = sorted(list(Path(self.dataset_dir).glob('[0-9]*')))
        self.samples_per_epoch = samples_per_epoch

    def __len__(self):
        return self.samples_per_epoch

    def read_audio_as_mono(self, sf: sf.SoundFile, num_frames: int):
        # Workaround for loading flac files
        if sf.name.endswith('.flac'):
            audio = sf.read(frames=num_frames, dtype='int32')
            audio = (audio / (2 ** (31) - 1)).astype(np.float32)
        else:
            audio = sf.read(frames=num_frames, dtype='float32')
        
        # If multichannel audio take a single channel
        if len(audio.shape) > 1:
            audio = audio[:, 0]

        return audio
    
    def sample_snippet(self, audio_file, rng):
        try:
            with sf.SoundFile(audio_file) as f:
                file_sr = f.samplerate
                
                assert file_sr == self.sr, "File sampling rate doesn't match expected sampling rate!"
                
                tgt_samples = int(round(file_sr * self.duration))
                num_frames = f.frames
        
                if tgt_samples > num_frames:
                    # Read entire audio
                    audio = self.read_audio_as_mono(f, num_frames=num_frames)
        
                    # Pad zeros to get to target duration
                    remain = tgt_samples - num_frames
                    pad_front = rng.randint(0, remain)
                    audio = np.pad(audio, (pad_front, remain - pad_front))
        
                else:
                    # Randomly choose start of snippet
                    start_frame = rng.randint(0, num_frames - tgt_samples + 1)
        
                    # Move to start of snippet and read
                    f.seek(start_frame)
                    audio = self.read_audio_as_mono(f, num_frames=tgt_samples)
        
                assert audio.shape[-1] == tgt_samples, f"Number of samples in audio incorrect.\
                                                            Expceted {audio.shape[-1]} found {tgt_samples}."
        except:
            print("ERROR AT FILE: ", audio_file, start_frame)
            return self.sample_snippet(audio_file, rng)
        return audio

    # def convolve_rir_source(self, src_idx):
    #     convolved_rcv = []
    #     for rcv_idx in range(self.rirs.shape[1]):
    #         au = torchaudio.functional.fftconvolve(self.source_audios[src_idx, :], self.rirs[src_idx, rcv_idx,:])
    #         convolved_rcv.append(au[:self.tgt_samples])
    #     return convolved_rcv

    def __getitem__(self, idx):
        '''
        Task : Get the source data from the libritts, generate RIR by calling gpuRIRsimulateRIR.py,
        store the resulting array in inputs and the target in outputs.
        
        '''
        if self.split=='train':
            master_path = "/mmfs1/gscratch/intelligentsystems/common_datasets/spatial_audio/train"
        elif self.split=='val':
            master_path = "/mmfs1/gscratch/intelligentsystems/common_datasets/spatial_audio/dev"
        else:
            master_path = "/mmfs1/gscratch/intelligentsystems/common_datasets/spatial_audio/test"
        if self.split == 'train':
            # IT IS ACTUALLY **** EXTREMELY **** IMPORTANT TO ADD IDX, ESPECIALLY IF WE ARE FIXING THE WORKERS SEEDS
            # OTHERWISE ALL WORKERS WILL HAVE THE SAME SEED!!!
            seed = idx + np.random.randint(1000000) 
        else:
            seed = idx

        # print(seed)
        
        rng = np.random.RandomState(seed)

        source_files = rng.choice(self.samples, size=2)
        #idx_1 = idx
        #idx_2 = rng.randint(self.samples_per_epoch)
        #idxs = [idx_1, idx_2]
        #source_files = [self.samples[i] for i in idxs]


        sim = PRASimulator(np_random_state = rng)
        rirs=sim.RIR()
        # rir_idx = rng.randint(1,50000)
        
        # rir_path = "/mmfs1/gscratch/intelligentsystems/common_datasets/spatial_audio_rirs"
        # rirs_files_wav = os.listdir(rir_path)
        # rir_files = [file for file in rirs_files_wav if file.startswith(f"{rir_idx}_") and file.endswith(".wav")]
        # rir_src_1, sr = torchaudio.load(os.path.join(rir_path, rir_files[0]))
        # rir_src_2, sr = torchaudio.load(os.path.join(rir_path, rir_files[1]))
        # rirs = torch.cat((rir_src_1.unsqueeze(0), rir_src_2.unsqueeze(0)), dim=0)







        # source_audio, sample_rate = torchaudio.load(source_file)
        #-------------------------------------------------------------
        # source_audios = []
        # for ind in range(len(source_files)):
        #     source_audio = self.sample_snippet(source_files[ind], rng)
        #     source_audios.append(source_audio)
        # source_audios = np.array(source_audios)
        source_audios = torch.stack([torch.from_numpy(self.sample_snippet(source_files[ind], rng)) for ind in range(len(source_files))])
        sample_rate = self.sr

        # print(" audio shape", source_audios.shape)
        # print("sample rate is", sample_rate)
        tgt_samples = int(np.round(self.sr*self.duration))
        convolved_audio = torch.zeros((rirs.shape[0], rirs.shape[1], tgt_samples))
        #for a single audio file in both the source microphones
#-----------------------------------------------------------------------------------
        for src_idx in range(rirs.shape[0]):
            for rcv_idx in range(rirs.shape[1]):
                au = scipy.signal.convolve(source_audios[src_idx, :], rirs[src_idx, rcv_idx,:])
                convolved_audio[src_idx][rcv_idx] = torch.from_numpy(au[:tgt_samples])
#-----------------------------------------------------------------------------------
        # breakpoint()
        # source_audios_extended = source_audios.unsqueeze(1) #(nb_src, 1, T)
        # source_audios_tiled =  torch.tile(source_audios_extended, (1,4,1)) # (nb_src,4,T)
        # convolved_audio_transposed = torch.from_numpy(scipy.ndimage.convolve(rirs, source_audios_tiled)) #(nb_src, T, nb_rcv)
        # convolved_audio = torch.transpose(convolved_audio_transposed, 1, 2) #(nb_src, nb_rcv, T )
        # convolved_audio = convolved_audio[:,:,:tgt_samples] #(nb_src, nb_rcv, T(5s duration) )

                #au = au[:tgt_samples]
                #microphones.append(au)
            # convolved_audio[src_idx] = torch.stack(microphones)
        mixture_audio = torch.sum(convolved_audio, dim=0) #(1, nb_rcv, fs*T)
        mixture_audio = mixture_audio.unsqueeze(0)
        mixture_audio_tr = torch.transpose(mixture_audio[0, :, :],0,1)
        assert mixture_audio_tr.shape[0]==120000
        sf.write(f"{master_path}/{idx+1}.wav", mixture_audio_tr, self.sr)

if __name__=='__main__':
    dataset_dir, sr, split = '/scr/LibriTTS/test-clean', 24000, 'test'
    libritts = LibriTTS_spatial_audio(dataset_dir=dataset_dir, sr=sr, split = split)
    for idx in tqdm.tqdm(range(200)):
        libritts.__getitem__(idx)


        






                